1.卷积的特点 
	局部连接 权值共享，下载样
2不同的卷积层都提取什么类型特征
	浅层：提取边缘特征
	中层：提取局部特征
	深层：提取全局特征
3.卷积的size变化
	(h - k + p + s)/s
4.1x1卷积的作用
	1.对特征图的通道数进行升维或降维，降维时减少模型参数量
	2.1x1卷积＋激活函数， 增加网络的非线性，提升网络表达能力
5.转置卷积的作用
	对特征图进行上采样或扩张
		1.语义分割
		2.物体检测中需要输出与原图像大小一致的图
	size： w = s*(n - 1) + k - 2p  (p为单边padding )
6.转置卷积的棋盘效应是什么
	原因： 转置卷积的不均匀叠加导致的图像中某个部位的颜色比其他部分更深
	解决办法： 双线性插值 和 近邻插值
7.空洞卷积的作用
	在不进行池化操作损失信息情况下，增大感受野，让每个卷积输出都包含较大范围的信息
	获取多尺度上下文信息，当带有不同扩展率的空洞卷积核叠加时， 不同感受野会带来多尺度信息
	卷积核size变化：k = k + (k - 1)*(d - 1)
8.感受野的作用
	一般来说感受野越大越好
	感受野足够大时，忽略的信息就较少
	目标检测任务中，设置 的anchor要对其感受野， anchor太大或偏离感受野会对性能产生影响
9.感受野的计算
	RF_n = RF_n-1 + （k - 1）* (stride的乘积)
10.如何增大感受野
	1.使用空洞卷积
	2.使用池化层
	3.增大卷积核
11.简述分组卷积
	1.分组卷积最早出现于alexnet，用于切分网络，使得能够在多个GPU上并行运算
	好处：1-减少参数量
		2-可以堪称稀疏操作，可以在较少的参数量情况下可以获得更好的效果
	缺点：对内存的访问成都并未降低，且现有gpu加速库优化成都有限，因此效率提升的效果并不如理论上
12.简述常见的激活函数， sigmoid， tanh， relu
13.如何提升CNN模型的泛化能力
	1.采集更多数据
	2.优化数据分布，使得数据类别均衡
	3.数据增强
	4.优化网络结构
	5.选用合适优化器
14.BN层是什么，作用是什么，用在哪里
	批量归一化层用在卷积和激活函数之间
	作用：	可以选用较大的学习率，用于加速收敛
		使得分布更稳定
		防止过拟合
		解决梯度消失和梯度爆炸
	
15.BN层训练时为什么不使用整个数据集的均值和方差
	容易出现过拟合问题
16.如何解决类别不平衡问题
	1.过采样
	2.欠采样
	3.类别权重，给予少量类的的数据更高的权重
	4.数据增强
17.简述常见的优化器
	1.随机梯度下降SGD
	2.自适应梯度下降 Aaptive
	3.Adam
	SGD缺点是其更新方向完全依赖于当前batch计算出的梯度，因而十分不稳定。

	Adam的优点主要在于：

	考虑历史步中的梯度更新信息，能够降低梯度更新噪声。此外经过偏差校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。
	但是Adam也有其自身问题：可能会对前期出现的特征过拟合，后期才出现的特征很难纠正前期的拟合效果。二者似乎都没法很好避免局部最优问题。


23.yolov1的loss是什么，，yolov1的缺点
	1.由于YOLOv1每个网格的检测框只有2个，对于密集型目标检测和小物体检测都不能很好适用。
	2.Inference时，当同一类物体出现的不常见的长宽比时泛化能力偏弱。
	3.由于损失函数的问题，定位误差是影响检测效果的主要原因，尤其是大小物体的处理上，还有待加强。
24.yolov2中采用什么训练方法？多尺度训练的逻辑是什么
	多尺度训练（Multi-Scale Training）的逻辑是模型每训练一定的Epoch，改变输入图片的尺寸，使得模型对不同的输入尺寸更鲁棒，能够从容地对不同尺寸的图像进行检测。图像尺寸使用32的倍数
25.yolov2中loss是什么