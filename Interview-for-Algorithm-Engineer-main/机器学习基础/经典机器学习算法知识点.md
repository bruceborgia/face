# 目录

- [1.K-means算法逻辑？](#user-content-1.k-means算法逻辑？)
- [2.K近邻算法逻辑？](#user-content-2.k近邻算法逻辑？)

<h2 id="1.k-means算法逻辑？">1.K-means算法逻辑？</h2>

K-means算法是<font color=DeepSkyBlue>一个实用的无监督聚类算法，其聚类逻辑依托欧式距离，当两个目标的距离越近，相似度越大</font>。对于给定的样本集，按照样本之间的距离大小，将样本集划分为 $K$ 个簇。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。

**K-means的主要算法步骤**：

1. 选择初始化的 $k$ 个样本作为初始聚类中心 $D = \{ D_{1}, D_{2}, D_{3}, ..., D_{k} \}$ 。
2. 针对数据集中每个样本 $x_{i}$ ，计算它到 $k$ 个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中.
3. 针对每个类别 $D_{j}$ ，重新计算它的聚类中心 $D_{j} = \frac{1}{|c_{j}|}\sum_{x\in c_{j}}x$ 。（即属于该类的所有样本的质心）；
4. 重复上面2和3两步的操作，直到达到设定的中止条件（迭代次数、最小误差变化等）。

**K-Means的主要优点**：

1. 原理简单，实现容易，收敛速度快。
2. 聚类效果较优。
3. 算法的可解释度比较强。
4. 主要需要调参的参数仅仅是簇数k。

**K-Means的主要缺点**：

1. K值需要人为设定，不好把握。
2. 对初始的簇中心敏感，不同选取方式会得到不同结果。
3. 对于不是凸的数据集比较难收敛。
4. 如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。
5. 迭代结果只是局部最优。
6. 对噪音和异常点比较的敏感。

<h2 id="2.k近邻算法逻辑？">2.K近邻算法逻辑？</h2>

K近邻（K-NN）算法<font color=DeepSkyBlue>计算不同数据特征值之间的距离进行分类</font>。存在一个样本数据集合，也称作训练数据集，并且数据集中每个数据都存在标签，即我们知道每一个数据与所属分类的映射关系。接着输入没有标签的新数据后，在训练数据集中找到与该新数据最邻近的K个数据，然后提取这K个数据中占多数的标签作为新数据的标签<font color=DeepSkyBlue>（少数服从多数逻辑）</font>。

**K近邻算法的主要步骤**：

1. 计算新数据与各个训练数据之间的距离。
2. 按照距离的递增关系进行排序。
3. 选取距离最小的K个点。
4. 确定前K个点所在类别的出现频率。
5. 返回前K个点中出现频率最高的类别作为新数据的预测分类。

![](https://files.mdnice.com/user/33499/1923b502-8c67-491e-9fb4-402c4e29bfdb.png)

K近邻算法的结果很大程度取决于K的选择。其距离计算一般使用欧氏距离或曼哈顿距离等经典距离度量。

**K近邻算法的主要优点**：

1. 理论成熟，思想简单，既可以用来做分类又可以做回归。
2. 可以用于非线性分类。
3. 对数据没有假设，准确度高，对异常点不敏感。
4. 比较适用于数据量比较大的场景，而那些数据量比较小的场景采用K近邻算法算法比较容易产生误分类情况。

**K近邻算法的主要缺点**：

1. 计算复杂性高；空间复杂性高。
2. 样本不平衡的时候，对稀有类别的预测准确率低。
3. 是慵懒散学习方法，基本上不学习，导致预测时速度比起逻辑回归之类的算法慢。
4. 可解释性不强。
