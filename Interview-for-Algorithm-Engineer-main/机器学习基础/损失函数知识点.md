# 目录

- [1.Softmax的定义和作用](#user-content-1.softmax的定义和作用)
- [2.交叉熵定义和作用](#user-content-2.交叉熵定义和作用)
- [3.格拉姆矩阵的相关概念？](#user-content-3.格拉姆矩阵的相关概念？)
- [4.感知损失的相关概念?](#user-content-4.感知损失的相关概念)
- [5.KL散度相关概念](#user-content-5.kl散度相关概念)
- [6.JS散度相关概念](#user-content-6.js散度相关概念)

<h2 id="1.softmax的定义和作用">1.Softmax的定义和作用</h2>
  
在二分类问题中，我们可以使用sigmoid函数将输出映射到【0，1】区间中，从而得到单个类别的概率。当我们将问题推广到多分类问题时，可以使用Softmax函数，对输出的值映射为概率值。
  
![](https://files.mdnice.com/user/33499/ba15fa57-2ef3-4a47-9536-4bc277428f01.png)

其定义为：

![](https://files.mdnice.com/user/33499/526a9221-7dae-4f45-99ae-7d9f77dc3aa3.png)

其中a代表了模型的输出。

<h2 id="2.交叉熵定义和作用">2.交叉熵定义和作用</h2>

交叉熵（cross entropy）常用于深度学习中的分类任务，其可以表示预测值与ground truth之间的差距。

交叉熵是信息论中的概念。其定义为：
  
![](https://files.mdnice.com/user/33499/7c811f76-ef47-42b9-93b2-25517e7463ed.png)

$P$ 代表 $gt$ 的概率分布， $q$ 代表预测值的概率分布。交叉熵从相对熵（KL散度）演变而来， $log$ 代表了信息量， $q$ 越大说明可能性越大，其信息量越少；反之则信息量越大。通过不断的训练优化，逐步减小交叉熵损失函数的值来达到缩小 $p$ 和 $q$ 距离的目的。


<h2 id="3.格拉姆矩阵的相关概念？">3.格拉姆矩阵的相关概念？</h2>

n维欧式空间中任意k个向量之间两两的内积所组成的矩阵，称为这k个向量的格拉姆矩阵(Gram matrix)，这是一个对称矩阵。 

![](https://files.mdnice.com/user/33499/a850fa8d-7ae5-415d-89ad-230836e01a49.png)

![](https://files.mdnice.com/user/33499/08889876-98b7-4a2f-ad50-7ea4eb7128a7.png)

其中对角线元素提供了k个不同特征图（a1，a2 ... ，ak）各自的信息，其余元素提供了不同特征图之间的相关信息。既能体现出有哪些特征，又能体现出不同特征间的紧密程度。图像风格迁移领域将其定义为风格特征。

格拉姆矩阵在风格迁移中有广泛的应用，深度学习中经典的风格迁移流程是：

1. 准备基线图像和风格图像。

2. 使用特征提取器分别提取基线图像和风格图像的feature map。

3. 分别计算两个图像的feature map的格拉姆矩阵，以两个图像的格拉姆矩阵的差异最小化为优化目标，不断调整基线图像，使风格不断接近目标风格图像。

<h2 id="4.感知损失的相关概念">4.感知损失的相关概念?</h2>

感知损失在图像生成领域中比较常用。其核心是将gt图片卷积得到的高层feature与生成图片卷积得到的高层feature进行回归，从而约束生成图像的高层特征（内容和全局结构）。

![经典感知损失结构](https://files.mdnice.com/user/33499/2c510820-c674-4e92-bd10-a742f1d1218f.png)

上面的公式中，划线部分代表了高层特征，一般使用VGG作为特征提取器。

<h2 id="5.kl散度相关概念">5.KL散度相关概念</h2>

KL散度（Kullback-Leibler divergence），可以以称作相对熵（relative entropy）或信息散度（information divergence）。<font color=DeepSkyBlue>KL散度的理论意义在于度量两个概率分布之间的差异程度，当KL散度越大的时候，说明两者的差异程度越大；而当KL散度小的时候，则说明两者的差异程度小</font>。如果两者相同的话，则该KL散度应该为0。

接下来我们举一个具体的🌰：

我们设定两个概率分布分别为 $P$ 和 $Q$ ，在设定为连续随机变量的前提下，他们对应的概率密度函数分别为 $p(x)$ 和 $q(x)$ 。如果我们用 $q(x)$ 去近似 $p(x)$ ，则KL散度可以表示为：

$$KL(P||Q) = \int p(x)\log \frac{p(x)}{q(x)}dx $$

从上面的公式可以看出，当且仅当 $P=Q$ 时， $KL(P||Q) = 0$ 。此外我们可以知道<font color=DeepSkyBlue>KL散度具备非负性</font>，即 $KL(P||Q) >= 0$ 。并且从公式中我们也发现，<font color=DeepSkyBlue>KL散度不具备对称性</font>，也就是说 $P$ 对于 $Q$ 的KL散度并不等于 $Q$ 对于 $P$ 的KL散度。因此，**KL散度并不是一个度量（metric），即KL散度并非距离**。

我们再来看看离散的情况下用 $q(x)$ 去近似 $p(x)$ 的KL散度的公式：

$$KL(P||Q) = \sum p(x)\log \frac{p(x)}{q(x)} $$

接下来我们对上面的式子进行展开：

$$KL(P||Q) = \sum p(x)\log \frac{p(x)}{q(x)}  = -\sum p(x)\log(q(x)) + \sum p(x)\log(p(x)) = H(P,Q) - H(P)$$

最后得到的第一项称作 $P$ 和 $Q$ 的交叉熵（cross entropy），后面一项就是熵。

在信息论中，熵代表着信息量， $H(P)$ 代表着基于 $P$ 分布自身的编码长度，也就是最优的编码长度（最小字节数）。而 $H(P,Q)$ 则代表着用 $Q$ 的分布去近似 $P$ 分布的信息，自然需要更多的编码长度。并且两个分布差异越大，需要的编码长度越大。所以两个值相减是大于等于0的一个值，代表冗余的编码长度，也就是两个分布差异的程度。所以<font color=DeepSkyBlue>KL散度在信息论中还可以称为相对熵（relative entropy）</font>。

对深度学习中的生成模型来说，我们希望最小化真实数据分布与生成数据分布之间的KL散度，从而使得生成数据尽可能接近真实数据的分布。在实际场景中，我们是几乎不可能知道真实数据分布 $P_{data}(x)$ 的，我们使用训练数据形成的生成分布在逼近 $P_{data}(x)$ 。

<h2 id="6.js散度相关概念">6.JS散度相关概念</h2>

JS散度全称Jensen-Shannon散度，简称JS散度。在概率统计中，<font color=DeepSkyBlue>JS散度也与KL散度一样具备了测量两个概率分布相似程度的能力，它的计算方法基于KL散度，继承了KL散度的非负性等，但有一点重要的不同，JS散度具备了对称性</font>。

JS散度的公式如下所示，我们设定两个概率分布为 $P$ 和 $Q$ ，另外我们还设定 $M = 0.5 \times (P + Q)$ ，KL为KL散度公式。

$$JSD(P||Q) = \frac{1}{2}KL(P||M) + \frac{1}{2}KL(Q||M) $$

如果我们把KL散度公式写入展开的话，结果如下所示：

$$JSD(P||Q) = \int p(x)\log \frac{p(x)}{\frac{p(x) +q(x)}{2}} dx+ \int q(x)\log \frac{q(x)}{\frac{p(x) +q(x)}{2}}dx$$

深度学习中使用KL散度和JS散度进行度量的时候存在一个问题：

如果两个分布 $P$ ， $Q$ 离得很远，完全没有重叠的时候，那么KL散度值是没有意义的，而JS散度值是一个常数 $\log2$ 。这对以梯度下降为基础的深度学习算法有很大影响，这意味梯度为0，即梯度消失。
