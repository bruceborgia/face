# 目录

- [1.数据类别不平衡怎么处理？](#user-content-1.数据类别不平衡怎么处理？)
- [2.什么是过拟合，解决过拟合的方法有哪些？](#user-content-2.什么是过拟合，解决过拟合的方法有哪些？)
- [3.什么是欠拟合，解决欠拟合的方法有哪些？](#user-content-3.什么是欠拟合，解决欠拟合的方法有哪些？)
- [4.正则化的本质以及常用正则化手段？](#user-content-4.正则化的本质以及常用正则化手段？)
- [5.L范数的作用？](#user-content-5.l范数的作用？)
- [6.Dropout的作用？](#user-content-6.dropout的作用？)
- [7.如何找到让F1最高的分类阈值？](#user-content-7.如何找到让f1最高的分类阈值？)
- [8.L1正则为什么比L2正则更容易产生稀疏解?](#user-content-8.l1正则为什么比l2正则更容易产生稀疏解)
- [9.梯度爆炸和梯度消失产生的原因及解决方法?](#user-content-9.梯度爆炸和梯度消失产生的原因及解决方法)
- [10.数据EDA逻辑（Exploratory Data Analysis）？](#user-content-10.数据eda逻辑（exploratory-data-analysis）？)

<h2 id="1.数据类别不平衡怎么处理？">1.数据类别不平衡怎么处理？</h2>
  
1. 数据增强。

2. 对少数类别数据做过采样，多数类别数据做欠采样。

3. 损失函数的权重均衡。（不同类别的loss权重不一样，最佳参数需要手动调节）

4. 采集更多少数类别的数据。

5. 转化问题定义，将问题转化为异常点检测或变化趋势检测问题。 异常点检测即是对那些罕见事件进行识别，变化趋势检测区别于异常点检测，其通过检测不寻常的变化趋势来进行识别。

6. 使用新的评价指标。

7. 阈值调整，将原本默认为0.5的阈值调整到：较少类别/（较少类别+较多类别）。

<h2 id="2.什么是过拟合，解决过拟合的方法有哪些？">2.什么是过拟合，解决过拟合的方法有哪些？</h2>
  
<font color=DeepSkyBlue>过拟合</font>：模型在训练集上拟合的很好，但是模型连噪声数据的特征都学习了，丧失了对测试集的泛化能力。

<font color=DeepSkyBlue>解决过拟合的方法</font>：

1. 重新清洗数据，数据不纯会导致过拟合，此类情况需要重新清洗数据或重新选择数据。
  
2. 增加训练样本数量。使用更多的训练数据是解决过拟合最有效的手段。我们可以通过一定的规则来扩充训练数据，比如在图像分类问题上，可以通过图像的平移、旋转、缩放、加噪声等方式扩充数据;也可以用GAN网络来合成大量的新训练数据。
3. 降低模型复杂程度。适当降低模型复杂度可以避免模型拟合过多的噪声数据。在神经网络中减少网络层数、神经元个数等。
4. 加入正则化方法，增大正则项系数。给模型的参数加上一定的正则约束，比如将权值的大小加入到损失函数中。
5. 采用dropout方法，dropout方法就是在训练的时候让神经元以一定的概率失活。
6. 提前截断（early stopping），减少迭代次数。
7. 增大学习率。
8. 集成学习方法。集成学习是把多个模型集成在一起，来降低单一模型的过拟合风险，如Bagging方法。

<h2 id="3.什么是欠拟合，解决欠拟合的方法有哪些？">3.什么是欠拟合，解决欠拟合的方法有哪些？</h2>
  
<font color=DeepSkyBlue>欠拟合</font>：模型在训练集和测试集上效果均不好，其根本原因是模型没有学习好数据集的特征。

<font color=DeepSkyBlue>解决欠拟合的方法</font>：

1. 可以增加模型复杂度。对于神经网络可以增加网络层数或者神经元数量。

2. 减小正则化系数。正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要有针对性地减小正则化系数。

3. Boosting。

<h2 id="4.正则化的本质以及常用正则化手段？">4.正则化的本质以及常用正则化手段？</h2>
  
正则化是机器学习的核心主题之一。<font color=DeepSkyBlue>正则化本质是对某一问题加以先验的限制或约束以达到某种特定目的的一种操作</font>。在机器学习中我们通过使用正则化方法，防止其过拟合，降低其泛化误差。

常用的正则化手段：

1. 数据增强
  
2. 使用L范数约束
3. dropout
4. early stopping
5. 对抗训练

<h2 id="5.l范数的作用？">5.L范数的作用？</h2>
  
L范数主要起到了正则化（<font color=DeepSkyBlue>即用一些先验知识约束或者限制某一抽象问题</font>）的作用，而正则化主要是防止模型过拟合。

范数主要用来表征高维空间中的距离，故在一些生成任务中也直接用L范数来度量生成图像与原图像之间的差别。

下面列出深度学习中的范数：
  
![](https://files.mdnice.com/user/33499/bca9e394-0548-4882-8e7f-47607bebbba7.png)

![](https://files.mdnice.com/user/33499/5c98ba3a-d4d1-4078-9ee9-5146990e3363.png)


<h2 id="6.dropout的作用？">6.Dropout的作用？</h2>

Dropout是在训练过程中以一定的概率使神经元失活，也就是输出等于0。从而提高模型的泛化能力，减少过拟合。
  
![使用Dropout](https://files.mdnice.com/user/33499/5f3b5d37-facd-4817-a765-990e6833b4a7.png)

我们可以从<font color=DeepSkyBlue>两个方面去直观地理解Dropout的正则化效果</font>：1）在Dropout每一轮训练过程中随机丢失神经元的操作相当于多个模型进行取平均，因此用于预测时具有vote的效果。2）减少神经元之间复杂的共适应性。当隐藏层神经元被随机删除之后，使得全连接网络具有了一定的稀疏化，从而有效地减轻了不同特征的协同效应。也就是说，有些特征可能会依赖于固定关系的隐含节点的共同作用，而通过Dropout的话，就有效地避免了某些特征在其他特征存在下才有效果的情况，增加了神经网络的鲁棒性。

<font color=DeepSkyBlue>Dropout在训练和测试时的区别</font>：Dropout只在训练时产生作用，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合风险。而在测试时，应该用整个训练好的模型，因此不需要Dropout。

<h2 id="7.如何找到让f1最高的分类阈值？">7.如何找到让F1最高的分类阈值？</h2>
  
首先，这个问题只存在于二分类问题中，对于多分类问题，只需要概率最高的那个预测标签作为输出结果即可。

F1值是综合了精准率和召回率两个指标对模型进行评价：
  
![](https://files.mdnice.com/user/33499/9ed8d70a-11fe-4e11-9bf1-6f7666965316.png)

一般设0.5作为二分类的默认阈值，但一般不是最优阈值。想要精准率高，一般使用高阈值，而想要召回率高，一般使用低阈值。在这种情况下，我们通常可以通过P-R曲线去寻找最优的阈值点或者阈值范围。

<h2 id="8.l1正则为什么比l2正则更容易产生稀疏解">8.L1正则为什么比L2正则更容易产生稀疏解?</h2>

我们首先可以设目标函数为 $L$ ，目标函数中的权值参数为 $w$ ，那么目标函数和权值参数的关系如下所示：

![](https://img-blog.csdnimg.cn/20200817112906402.png)

如上图所示，最优的 $w$ 在绿色的点处，而且 $w$ 非零。

我们首先可以使用L2正则进行优化，新的目标函数： $L + CW^{2}$ ，示意图如下蓝线所示：

![](https://img-blog.csdnimg.cn/20200817113043873.png)

我们可以看到，最优的 $w$ 出现在黄点处， $w$ 的绝对值减小了，更靠近横坐标轴，但是依然是非零的。

<font color=DeepSkyBlue>为什么是非零的呢？</font>

我们可以对L2正则下的目标函数求导：

![](https://files.mdnice.com/user/33499/6fff569d-11fa-4c0d-93e7-604f6aed387e.png)

我们发现，权重 $w$ 每次乘上的是小于1的倍数进行收敛，而且其导数在 $w=0$ 时没有办法做到左右两边导数异号，所以L2正则使得整个训练过程稳定平滑，但是没有产生稀疏性。

接下来我们使用L1正则，新的目标函数： $L + C|w|$ ，示意图如下粉线所示：

![](https://img-blog.csdnimg.cn/20200817115050210.png)

这里最优的 $w$ 就变成了0。因为保证使用L1正则后 $x=0$ 处左右两个导数异号，就能满足极小值点形成的条件。

我们来看看这次目标函数求导的式子：

![](https://img-blog.csdnimg.cn/20200817115308997.png)

可以看出L1正则的惩罚很大， $w$ 每次都是减去一个常数的线性收敛，所以L1比L2更容易收敛到比较小的值，而如果 $C > |f^{'}(0)|$ ，就能保证 $w = 0$ 处取得极小值。

上面只是一个权值参数 $w$ 。在深层网路中，L1会使得大量的 $w$ 最优值变成0，从而使得整个模型有了稀疏性。

<h2 id="9.梯度爆炸和梯度消失产生的原因及解决方法">9.梯度爆炸和梯度消失产生的原因及解决方法?</h2>

### 梯度爆炸和梯度消失问题

一般在深层神经网络中，我们需要预防梯度爆炸和梯度消失的情况。

梯度消失（gradient vanishing problem）和梯度爆炸（gradient exploding problem）一般随着网络层数的增加会变得越来越明显。

例如下面所示的含有三个隐藏层的神经网络，梯度消失问题发生时，接近输出层的hiden layer3的权重更新比较正常，但是前面的hidden layer1的权重更新会变得很慢，导致前面的权重几乎不变，仍然接近初始化的权重，<font color=DeepSkyBlue>这相当于hidden layer1没有学到任何东西，此时深层网络只有后面的几层网络在学习，而且网络在实际上也等价变成了浅层网络</font>。

![](https://img-blog.csdnimg.cn/2020071110042155.png)

### 产生梯度爆炸和梯度消失问题的原因

我们来看看看反向传播的过程：

（假设网络每一层只有一个神经元，并且对于每一层 $y_{i} = \sigma(z_{i}) = \sigma(w_{i}x_{i} + b_{i})$ ）

![](https://img-blog.csdnimg.cn/20200711101713569.png)

可以推导出：

![](https://img-blog.csdnimg.cn/20200711101729614.png)

而sigmoid的导数 $\sigma^{'}(x)$ 如下图所示：

![](https://img-blog.csdnimg.cn/20200711101845385.png)

可以知道， $\sigma^{'}(x)$的最大值是$\frac{1}{4}$ ，而我们初始化的权重 $|w|$ 通常都小于1，因此 $\sigma^{'}(x)|w| <= \frac{1}{4}$ ，而且链式求导层数非常多，不断相乘的话，最后的结果越来越小，趋向于0，就会出现梯度消失的情况。

梯度爆炸则相反， $\sigma^{'}(x)|w| > 1$ 时，不断相乘结果变得很大。

<font color=DeepSkyBlue>梯度爆炸和梯度消失问题都是因为网络太深，网络权重更新不稳定造成的，本质上是梯度方向传播的连乘效应。</font>

### 梯度爆炸和梯度消失的解决方法

1. 使用预训练加微调策略。
2. 进行梯度截断。
3. 使用ReLU、LeakyReLU等激活函数。
4. 引入BN层。
5. 使用残差结构。
6. 使用LSTM思想。

<h2 id="10.数据eda逻辑（exploratory-data-analysis）？">10.数据EDA逻辑（Exploratory Data Analysis）？</h2>

1. 导入相应的Modules（numpy，pandas，matplotlib，PIL等）
2. 阅读了解所有的数据文件（图片数据，类别文件，辅助文件等）
3. 数据类别特征分析（数据类别总数，数据类别的平衡度，数据尺寸，噪声数据等）
4. 数据可视化二次分析（直观了解不同类别的区别）
