# 目录

- [1.CNN中池化的作用](#user-content-1CNN中池化的作用)
- [2.全局池化的作用](#user-content-2全局池化的作用)
- [3.池化的分类](#user-content-池化的分类)
- [4.池化的进阶使用---SPP结构介绍](#user-content-池化的进阶使用---SPP结构介绍)

<h1 id="1CNN中池化的作用">1.CNN中池化的作用</h1>

池化层的作用是 `<font color=DeepSkyBlue>`对感受野内的特征进行选择，提取区域内最具代表性的特征，能够有效地减少输出特征数量，进而减少模型参数量 `</font>`。按操作类型通常分为最大池化(Max Pooling)、平均池化(Average Pooling)和求和池化(Sum Pooling)，它们分别提取感受野内最大、平均与总和的特征值作为输出，最常用的是最大池化和平均池化。

<h1 id="2全局池化的作用">2.全局池化的作用</h1>

全局池化主要包括全局平均池化和全局最大池化。

![全局最大池化](https://files.mdnice.com/user/33499/4a9a663c-49ba-4259-b4cf-5838ae1ff781.png)

![全局平均池化](https://files.mdnice.com/user/33499/f92a2877-ec95-485b-8882-ed6845cef9fd.png)

接下来，Rocky以全局平均池化为例，讲述其如何在深度学习网络中发挥作用。

刚才已经讲过，全局平均池化就是对最后一层卷积的特征图，每个通道求整个特征图的均值。如下图所示：

![全局平均池化](https://img-blog.csdnimg.cn/20200312000813310.png)

一般网络的最后会再接几个全连接层，但全局池化后的feature map相当于一像素，所以最后的全连接其实就成了一个加权相加的操作。这种结构比起直接的全连接更加直观，参数量大大幅下降，并且泛化性能更好：

![](https://img-blog.csdnimg.cn/2020031200241849.png)

全局池化的作用：

1.**降低信息冗余** ：

* 池化层有助于提取输入特征图中的主要信息，同时抑制次要信息。这种操作使得模型更专注于重要特征，减少冗余或不相关的特征，有利于模型的训练和泛化能力。

2.**特征降维与下采样** ：

* 池化操作导致输出特征图的尺寸减小，实现了特征降维和下采样的效果。这有助于减少计算量，并提高后续层对图像特征的感知范围，使得一个池化后的像素对应前面图片中的一个区域。

3.**特征压缩与网络简化** ：

* 池化层能够对特征图进行压缩，减少计算资源的消耗，简化网络结构，降低模型复杂度，有助于防止过拟合，提高模型的泛化能力。

4.**提升模型的不变性** ：

* 池化操作有助于提升模型对尺度、旋转和平移的不变性。经过池化后的特征图，在输入特征图的大小或旋转角度发生变化时，输出特征图的大小和旋转角度保持不变。这种不变性有助于提高模型的泛化能力和鲁棒性。

5.实现非线性。

![](image/池化层知识点/1701610823539.png)

<h1 id="3CNN中池化的作用">3.池化的分类</h1>

### A. 一般池化（General Pooling）：

在CNN中，池化层用于减小特征图的空间尺寸，以降低计算量并减少过拟合的可能性。最常见的池化操作有两种：

#### 平均池化（Average Pooling）：

* 计算图像区域的平均值作为该区域池化后的值。
* 能够抑制由于邻域内大小受限造成估计值方差增大的现象。
* 其特点是对于背景的保留效果更好。

#### 最大池化（Max Pooling）：

* 选取图像区域的最大值作为该区域池化后的值。
* 能够抑制网络参数误差造成估计均值偏移的现象。
* 其特点是更好地提取纹理信息。

#### 随机池化（Stochastic Pooling）：

* 根据概率对局部的值进行采样，采样结果便是池化结果。

### B. 重叠池化（Overlapping Pooling）：

在某些情况下，相邻的池化窗口之间可以有重叠区域。这种情况下一般会设置池化窗口的大小（size）大于步幅（stride）。

重叠池化的特点是相比于常规池化操作，它可以更充分地捕获图像特征，但也可能导致计算量增加。

这些池化方法是CNN中常用的技术手段，用于在保留重要信息的同时减少数据尺寸和参数量，从而改善模型的性能和泛化能力。

<h1 id="4池化的进阶使用---SPP结构介绍">4.池化的进阶使用---SPP结构介绍</h1>

论文名称：Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition
下载地址：https://arxiv.org/abs/1406.4729

空间金字塔池化（Spatial Pyramid Pooling，SPP）层的引入解决了在传统卷积神经网络（CNN）中需要固定输入图像尺寸的限制。传统的全连接层对于输入要求固定大小的特征向量，这意味着所有输入图像需要统一尺寸，通常需要进行裁剪或拉伸，导致图像失真。SPP层允许网络接受不同尺寸的输入图像，通过金字塔形状的池化区域对不同大小的特征图进行整合和提取特征。其作用在于将不同大小的特征图转换成固定大小的特征向量，使得在连接全连接层之前，所有输入都具有相同的大小，无需提前处理图像。这种灵活性提高了网络的适用性和泛化能力，使得模型能够更灵活地处理各种尺寸的输入。

![1701611347960](https://github.com/WeThinkIn/Interview-for-Algorithm-Engineer/blob/main/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/Images/WechatIMG144.jpg)

![1701611392168](https://github.com/WeThinkIn/Interview-for-Algorithm-Engineer/blob/main/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/Images/WechatIMG145.jpg)

SPP（空间金字塔池化）的显著特点有：

固定大小的输出：无论输入尺寸如何，SPP能够产生固定大小的输出，克服了全连接层要求固定长度输入的限制。

多个窗口的池化：SPP采用多个窗口的池化，使其能够在不同尺度下提取特征。

尺度不变性和特征一致性：可以处理不同纵横比和尺寸的输入图像，增强了模型的尺度不变性，降低了过拟合的风险。

其他特点包括：

多样性训练图像对网络收敛更容易：SPP允许训练使用不同尺寸的图像，相较于单一尺寸的训练图像，这种多样性训练更有利于网络的收敛。

独立于特定网络设计和结构：SPP可用作卷积神经网络的最后一层，不会影响网络结构，仅替换了原本的池化层。

适用于图像分类和目标检测：SPP不仅适用于图像分类，还可用于目标检测等任务，扩展了其应用领域。

SPP的这些特点使得它成为一个强大的工具，在处理不同尺寸、不同纵横比的图像时，保持固定长度特征向量的输出，提高了模型的灵活性和泛化能力。
