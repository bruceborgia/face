# 目录

- [1.深度学习中有哪些经典的优化器？](#user-content-1深度学习中有哪些经典的优化器？)

<h1 id="1深度学习中有哪些经典的优化器？">1.深度学习中有哪些经典的优化器？</h1>

### SGD（随机梯度下降）

随机梯度下降的优化算法在科研和工业界是很常用的。

<font color=DeepSkyBlue>很多理论和工程问题都能转化成对目标函数进行最小化的数学问题。</font>

举个例子：梯度下降（Gradient Descent）就好比一个人想从高山上奔跑到山谷最低点，用最快的方式奔向最低的位置。

SGD的公式：

![](https://img-blog.csdnimg.cn/20200802220806705.png#pic_center)

动量（Momentum）公式：

![](https://img-blog.csdnimg.cn/20200802220955480.png#pic_center)

基本的mini-batch SGD优化算法在深度学习取得很多不错的成绩。然而也存在一些问题需解决：

1. 选择恰当的初始学习率很困难。
2. 学习率调整策略受限于预先指定的调整规则。
3. 相同的学习率被应用于各个参数。
4. 高度非凸的误差函数的优化过程，如何避免陷入大量的局部次优解或鞍点。

### AdaGrad（自适应梯度）

AdaGrad优化算法（Adaptive Gradient，自适应梯度），它能够对每个不同的参数调整不同的学习率，对频繁变化的参数以更小的步长进行更新，而稀疏的参数以更大的步长进行更新。

AdaGrad公式：

![](https://img-blog.csdnimg.cn/20200802222758468.png#pic_center)

![](https://img-blog.csdnimg.cn/20200802222812287.png#pic_center)


$g_{t,i}$ 表示t时刻的 $\theta_{i}$ 梯度。

$G_{t,ii}$ 表示t时刻参数 $\theta_{i}$ 的梯度平方和。

与SGD的核心区别在于计算更新步长时，增加了分母：<font color=DeepSkyBlue>梯度平方累积和的平方根</font>。此项能够累积各个参数 $\theta_{i}$ 的历史梯度平方，频繁更新的梯度，则累积的分母逐渐偏大，那么更新的步长相对就会变小，而稀疏的梯度，则导致累积的分母项中对应值比较小，那么更新的步长则相对比较大。

AdaGrad能够自动为不同参数适应不同的学习率（平方根的分母项相当于对学习率α进进行了自动调整，然后再乘以本次梯度），大多数的框架实现采用默认学习率α=0.01即可完成比较好的收敛。

**优势：** 在数据分布稀疏的场景，能更好利用稀疏梯度的信息，比标准的SGD算法更有效地收敛。

**缺点：** 主要缺陷来自分母项的对梯度平方不断累积，随时间的增加，分母项越来越大，最终导致学习率收缩到太小无法进行有效更新。

### RMSProp
RMSProp结合梯度平方的指数移动平均数来调节学习率的变化。能够在不稳定的目标函数情况下进行很好地收敛。

计算t时刻的梯度：

![](https://img-blog.csdnimg.cn/20200802224130311.png#pic_center)

计算梯度平方的指数移动平均数（Exponential Moving Average）， $\gamma$ 是遗忘因子（或称为指数衰减率），依据经验，默认设置为0.9。

![](https://img-blog.csdnimg.cn/20200802224414890.png#pic_center)

梯度更新的时候，与AdaGrad类似，只是更新的梯度平方的期望（指数移动均值），其中 $\varepsilon = 10^{-8}$ ，避免除数为0。默认学习率 $\alpha = 0.001$ 。

![](https://img-blog.csdnimg.cn/20200802224711856.png#pic_center)

**优势：** 能够克服AdaGrad梯度急剧减小的问题，在很多应用中都展示出优秀的学习率自适应能力。尤其在不稳定(Non-Stationary)的目标函数下，比基本的SGD、Momentum、AdaGrad表现更良好。

### Adam

Adam优化器结合了AdaGrad和RMSProp两种优化算法的优点。对梯度的一阶矩估计（First Moment Estimation，即梯度的均值）和二阶矩估计（Second Moment Estimation，即梯度的未中心化的方差）进行综合考虑，计算出更新步长。

**Adam的优势：**

1. 实现简单，计算高效，对内存需求少。
2. 参数的更新不受梯度的伸缩变换影响。
3. 超参数具有很好的解释性，且通常无需调整或仅需很少的微调。
4. 更新的步长能够被限制在大致的范围内（初始学习率）。
5. 能自然地实现步长退火过程（自动调整学习率）。
6. 很适合应用于大规模的数据及参数的场景。
7. 适用于不稳定目标函数。
8. 适用于梯度稀疏或梯度存在很大噪声的问题。

**Adam的实现原理：**

![](https://img-blog.csdnimg.cn/20200802230838428.png)

计算t时刻的梯度：

![](https://img-blog.csdnimg.cn/20200802230926200.png#pic_center)

然后计算梯度的指数移动平均数， $m_{0}$ 初始化为0。

类似于Momentum算法，综合考虑之前累积的梯度动量。

$\beta_{1}$ 系数为指数衰减率，控制动量和当前梯度的权重分配，通常取接近于1的值。默认为0.9。

![](https://img-blog.csdnimg.cn/20200802232541831.png#pic_center)

接着，计算梯度平方的指数移动平均数， $v_{0}$ 初始化为0。

$\beta_{2}$ 系数为指数衰减率，控制之前的梯度平方的影响情况。默认为0.999。

类似于RMSProp算法，对梯度平方进行加权均值。

![](https://img-blog.csdnimg.cn/20200802233221851.png#pic_center)

由于 $m_{0}$ 初始化为0，会导致$m_{t}$偏向于0，尤其在训练初期阶段。

所以，此处需要对梯度均值$m_{t}$进行偏差纠正，降低偏差对训练初期的影响。

![](https://img-blog.csdnimg.cn/2020080223350261.png#pic_center)

同时 $v_{0}$ 也要进行偏差纠正：

![](https://img-blog.csdnimg.cn/20200802233536671.png#pic_center)

最后总的公式如下所示：

![](https://img-blog.csdnimg.cn/20200802233611955.png#pic_center)

其中默认学习率 $\alpha = 0.001$ ， $\varepsilon = 10^{-8}$ 避免除数变为0。

从表达式中可以看出，对更新的步长计算，能够从梯度均值和梯度平方两个角度进行自适应地调节，而不是直接由当前梯度决定。

**Adam的不足：**

虽然Adam算法目前成为主流的优化算法，不过在很多领域里（如计算机视觉的图像识别、NLP中的机器翻译）的最佳成果仍然是使用带动量（Momentum）的SGD来获取到的。
