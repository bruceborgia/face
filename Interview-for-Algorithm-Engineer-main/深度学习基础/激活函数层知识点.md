# 目录
- [1.激活函数的作用，常用的激活函数有哪些](#user-content-1激活函数的作用，常用的激活函数有哪些)

<h1 id="1激活函数的作用，常用的激活函数有哪些">1.激活函数的作用，常用的激活函数有哪些</h1>

### 激活函数的作用

激活函数可以引入非线性因素，提升网络的学习表达能力。

### 常用的激活函数

**Sigmoid 激活函数**

 函数的定义为：
 
 $$f(x) = \frac{1}{1 + e^{-x}}$$

如下图所示，其值域为 $(0,1)$ 。也就是说，输入的每个神经元、节点都会被缩放到一个介于 $0$ 和 $1$ 之间的值。

当 $x$ 大于零时输出结果会趋近于 $1$ ，而当 $x$ 小于零时，输出结果趋向于 $0$ ，由于函数的特性，<font color=DeepSkyBlue>经常被用作二分类的输出端激活函数</font>。

![](https://files.mdnice.com/user/33499/ef73d59a-0208-4c8d-96ba-16df5e1631d8.png)

Sigmoid的导数:

$$f^{'}(x)=(\frac{1}{1+e^{-x}})^{'}=\frac{1}{1+e^{-x}}\left( 1- \frac{1}{1+e^{-x}} \right)=f(x)(1-f(x))$$

当 $x=0$ 时， $f(x)'=0.25$ 。

Sigmoid的优点:
1. 平滑
2. 易于求导
3. 可以作为概率，辅助解释模型的输出结果

Sigmoid的缺陷:

1. 当输入数据很大或者很小时，函数的梯度几乎接近于0，这对神经网络在反向传播中的学习非常不利。
2. Sigmoid函数的均值不是0，这使得神经网络的训练过程中只会产生全正或全负的反馈。
3. 导数值恒小于1，反向传播易导致梯度消失。

![Sigmoid导数示意图，两边梯度几乎为0](https://files.mdnice.com/user/33499/b6aa3d37-0d24-40c9-b802-27596d67ec39.png)

**Tanh激活函数**

Tanh函数的定义为：

$$f(x) = Tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

如下图所示，值域为 $(-1,1)$ 。

![](https://files.mdnice.com/user/33499/e1570597-a9c0-4546-937b-33d3a237fd7e.png)

Tanh的优势：

1. Tanh函数把数据压缩到-1到1的范围，解决了Sigmoid函数均值不为0的问题，所以在实践中通常Tanh函数比Sigmoid函数更容易收敛。在数学形式上其实Tanh只是对Sigmoid的一个缩放形式，公式为 $tanh(x) = 2f(2x) -1$（ $f(x)$ 是Sigmoid的函数）。
2. 平滑
3. 易于求导

Tanh的导数:

$$f^{'}(x)=(\frac{e^x - e^{-x}}{e^x + e^{-x}})^{'}=1-(tanh(x))^2$$

当 $x=0$ 时，$f(x)'=1$ 。

由Tanh和Sigmoid的导数也可以看出Tanh导数更陡，收敛速度比Sigmoid快。

![Tanh导数示意图](https://files.mdnice.com/user/33499/6d4b89a4-2540-4965-bb22-f2c66c2f8245.png)

Tanh的缺点：

导数值恒小于1，反向传播易导致梯度消失。

**Relu激活函数**

Relu激活函数的定义为：

$$f(x) = max(0, x)$$  

如下图所示，值域为 $[0,+∞)$ 。

![](https://files.mdnice.com/user/33499/b8b05b3a-69d6-4f1d-9133-a188aafb8648.png)

ReLU的优势：

1. 计算公式非常简单，不像上面介绍的两个激活函数那样涉及成本更高的指数运算，大量节约了计算时间。
2. 在随机梯度下降中比Sigmoid和Tanh更加容易使得网络收敛。
3. ReLU进入负半区的时候，梯度为0，神经元此时会训练形成单侧抑制，产生稀疏性，能更好更快地提取稀疏特征。
4. Sigmoid和Tanh激活函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度消失，而ReLU函数大于0部分都为常数保持梯度不衰减，不会产生梯度消失现象。

<font color=DeepSkyBlue>稀疏</font>：在神经网络中，这意味着激活的矩阵含有许多0。这种稀疏性能让我们得到什么？这能提升时间和空间复杂度方面的效率，常数值所需空间更少，计算成本也更低。

ReLU的导数：

$$c(u)=\begin{cases} 0,x<0 \\ 1,x>0 \\ undefined,x=0\end{cases}$$

通常 $x=0$ 时，给定其导数为 $1$ 和 $0$ 。

![ReLU的导数](https://files.mdnice.com/user/33499/09c86709-52f4-4278-8949-d83a41f9aebd.png)

ReLU的不足:

1. 训练中可能会导致出现某些神经元永远无法更新的情况。其中一种对ReLU函数的改进方式是LeakyReLU。
2. ReLU不能避免梯度爆炸问题。

**LeakyReLU激活函数** 

LeakyReLU激活函数定义为： 

$$f(x) =  \left\{
   \begin{aligned}
   ax, \quad x<0 \\
   x, \quad x\ge0
   \end{aligned}
   \right.$$

如下图所示（ $a = 0.5$ ），值域为 $(-∞,+∞)$ 。 

![](https://files.mdnice.com/user/33499/d475ec3a-0f4d-4154-896a-278f0e87d39e.png)

LeakyReLU的优势:

该方法与ReLU不同的是在$x$小于0的时候取 $f(x) = ax$ ，其中$a$是一个非常小的斜率（比如0.01）。这样的改进可以使得当 $x$ 小于0的时候也不会导致反向传播时的梯度消失现象。

LeakyReLU的不足:

1. 无法避免梯度爆炸的问题。
2. 神经网络不学习 $\alpha$ 值。
3. 在求导的时候，两部分都是线性的。

**SoftPlus激活函数**

SoftPlus激活函数的定义为：

$$f(x) = ln( 1 + e^x)$$

值域为 $(0,+∞)$ 。

函数图像如下:

![](https://files.mdnice.com/user/33499/bf513661-17d8-4197-87c9-5002f77d7c86.png)

可以把SoftPlus看作是ReLU的平滑。

**ELU激活函数**

ELU激活函数解决了ReLU的一些问题，同时也保留了一些好的方面。这种激活函数要选取一个 $\alpha$ 值，其常见的取值是在0.1到0.3之间。

函数定义如下所示：

$$f(x) =  \left\{
   \begin{aligned}
   a(e^x -1), \quad x<0 \\
   x, \quad x\ge0
   \end{aligned}
   \right.$$

如果我们输入的 $x$ 值大于 $0$ ，则结果与ReLU一样，即 $y$ 值等于 $x$ 值；但如果输入的 $x$ 值小于 $0$ ，则我们会得到一个稍微小于 $0$ 的值，所得到的 $y$ 值取决于输入的 $x$ 值，但还要兼顾参数 $\alpha$ ——可以根据需要来调整这个参数。公式进一步引入了指数运算 $e^x$ ，因此ELU的计算成本比ReLU高。

下面给出了 $\alpha$ 值为0.2时的ELU函数图：

![ELU函数图](https://img-blog.csdnimg.cn/20200401154732541.png)

ELU的导数：

![ELU的导数公式](https://img-blog.csdnimg.cn/20200401155003365.png)

导数图如下所示：

![ELU的导数图](https://img-blog.csdnimg.cn/20200401155309599.png)

ELU的优势：

1. 能避免ReLU中一些神经元无法更新的情况。
2. 能得到负值输出。

ELU的不足：

1. 包含指数运算，计算时间长。
2. 无法避免梯度爆炸问题。
3. 神经网络无法学习 $\alpha$ 值。
